# HW-5
HW 5 â€“ One paragraph on a data normalization problem and commit.

Below just some coding practice:
<img src="http://www.totalgeek.co/images/computer.jpg" alt="totalgeek.com" style="width:5px;height:5px;">

"So," normalization is the infamous method of optimizing large databases accomplishing fast and simplified indexing by enforcing restraints with the goal of reducing/eliminating redundancies. But there are examples where normalization isn't the best choice. Intrinsically, normalization makes a program faster by freeing up space in your drives. But what if space isn't a concern? If the sacrifice of extra storage is having speed through multiple points to access data (and you have the access to extra storage) then why normalize? If efficiency reduces your speed then normalization itself becomes the restraint. Another major disadvantage of normalization is the amount of knowledge one must have in order to normalize a system. The system is only as good as the one who makes it, sort of thing. Furthermore, there's evidence that redundancies and gaps may not affect speed as one may think. According to Jeff Atwood of Coding Horror, even the most basic of today's computers are fast enough to sort through duplication in data. In fact, Atwood suggests to normalize AND denormalize, depending on what the functions call for.
